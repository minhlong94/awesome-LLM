# Awesome LLM
A collection of interesting papers related to Large Language Models (LLM)

## Models
- BLOOM: 176B params, 59 languages, open-access: https://bigscience.huggingface.co/blog/bloom
- NLLB (No Language Left Behind): 200 languages: https://ai.facebook.com/research/no-language-left-behind/
- GPT-NeoX: 20B params: https://github.com/EleutherAI/gpt-neox
- OPT: 66B and 175B params: https://github.com/facebookresearch/metaseq
- GODEL: 220M, 770M, 2.7B params: https://github.com/microsoft/GODEL
- Megatron-LM: 22B, 175B, 530B, 1T params: https://github.com/NVIDIA/Megatron-LM
- Turing-NLG: 17B: https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/
- DALL-E 2: https://openai.com/dall-e-2/
- iGPT: 76M, 445M, 1.4B params: https://openai.com/blog/image-gpt/
- GATO: A Generalist Agent: https://arxiv.org/abs/2205.06175, https://www.deepmind.com/publications/a-generalist-agent
- Gopher: 280B params: https://www.deepmind.com/blog/language-modelling-at-scale-gopher-ethical-considerations-and-retrieval
- PaLM: 540B params: https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html
- GLaM: 1.2T params: https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html
- LaMDA: 137B params: https://ai.googleblog.com/2022/01/lamda-towards-safe-grounded-and-high.html


## Speedup tricks, Scaling Laws, and Dataset Analysis
- Training Compute-Optimal Large Language Models: https://arxiv.org/abs/2203.15556
- Scaling Laws for Neural Language Models: https://arxiv.org/abs/2001.08361v1
- Prune Once for All: Sparse Pre-Trained Language Models: https://arxiv.org/abs/2111.05754
- Multimodal datasets: misogyny, pornography, and malignant stereotypes: https://arxiv.org/abs/2110.01963

## Libraries
- DeepSpeed: https://github.com/microsoft/DeepSpeed
- Composer: https://github.com/mosaicml/composer
- DeepSparse: https://github.com/neuralmagic/deepsparse
